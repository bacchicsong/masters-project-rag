pip install requests beautifulsoup4 lxml tqdm colorama

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import json
import shutil
from pathlib import Path
from typing import Set, List, Dict
from tqdm import tqdm
from colorama import init, Fore, Style
import re

init(autoreset=True)


# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================================================

class ParserConfig:
    BASE_URL = "https://www.tbank.ru/invest/help/educate/how-it-works/"
    OUTPUT_DIR = "tbank_knowledge"
    
    MAX_RECURSION_DEPTH = 4
    
    URL_INCLUDE_PATTERNS = ['/invest/help/']
    URL_EXCLUDE_PATTERNS = ['/login', '/logout', '/api/', '.pdf', '.jpg', '.png', '#']
    
    REQUEST_DELAY = 1.0
    REQUEST_TIMEOUT = 10
    
    MIN_PARAGRAPH_LENGTH = 10
    
    CHECKPOINT_ENABLED = True
    CHECKPOINT_INTERVAL = 10
    
    USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'


# ============================================================================
# –ü–ê–†–°–ï–†
# ============================================================================

class TBankKnowledgeParser:
    def __init__(self, config: ParserConfig = ParserConfig):
        self.config = config
        self.output_dir = Path(config.OUTPUT_DIR)
        self.output_dir.mkdir(exist_ok=True)
        
        self.visited_urls: Set[str] = set()
        self.articles: List[Dict] = []
        self.failed_urls: List[str] = []
        
        self.headers = {'User-Agent': config.USER_AGENT}
        self.allowed_domain = urlparse(config.BASE_URL).netloc
        
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_links_found': 0,
            'total_articles': 0,
            'total_chars': 0,
            'start_time': None
        }
        
        print(f"\n{Fore.CYAN}{'='*80}")
        print(f"{Fore.CYAN}üöÄ T-Bank Knowledge Parser")
        print(f"{Fore.CYAN}{'='*80}{Style.RESET_ALL}")
        print(f"{Fore.GREEN}üìÅ {self.output_dir}")
        print(f"{Fore.GREEN}üåê {config.BASE_URL}")
        print(f"{Fore.YELLOW}üìä –ì–ª—É–±–∏–Ω–∞: {config.MAX_RECURSION_DEPTH}\n")
    
    def get_page_content(self, url: str) -> BeautifulSoup:
        self.stats['total_requests'] += 1
        
        try:
            print(f"{Fore.YELLOW}‚¨áÔ∏è  {url}")
            response = requests.get(url, headers=self.headers, timeout=self.config.REQUEST_TIMEOUT)
            response.raise_for_status()
            
            self.stats['successful_requests'] += 1
            print(f"{Fore.GREEN}‚úÖ {len(response.content)} –±–∞–π—Ç")
            
            return BeautifulSoup(response.content, 'html.parser')
        except Exception as e:
            self.stats['failed_requests'] += 1
            self.failed_urls.append(url)
            print(f"{Fore.RED}‚ùå {str(e)[:100]}")
            return None
    
    def is_valid_url(self, url: str) -> bool:
        for pattern in self.config.URL_EXCLUDE_PATTERNS:
            if pattern in url:
                return False
        for pattern in self.config.URL_INCLUDE_PATTERNS:
            if pattern in url:
                return True
        return False
    
    def extract_links(self, soup: BeautifulSoup, current_url: str) -> Set[str]:
        links = set()
        if not soup:
            return links
        
        all_links = soup.find_all('a', href=True)
        
        for link in tqdm(all_links, desc="–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫", leave=False):
            href = link['href']
            full_url = urljoin(current_url, href)
            
            if urlparse(full_url).netloc != self.allowed_domain:
                continue
            
            if not self.is_valid_url(full_url):
                continue
            
            if full_url not in self.visited_urls:
                links.add(full_url)
        
        self.stats['total_links_found'] += len(links)
        print(f"{Fore.GREEN}   –ù–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫: {len(links)}")
        
        return links
    
    def extract_article_content(self, soup: BeautifulSoup, url: str) -> Dict:
        if not soup:
            return None
        
        article_data = {
            'url': url,
            'title': '',
            'content': '',
            'sections': []
        }
        
        title = soup.find('h1')
        if title:
            article_data['title'] = title.get_text(strip=True)
        
        content_selectors = [
            {'class': 'article-content'},
            {'class': 'content'},
            {'role': 'main'},
            {'class': 'text-content'},
        ]
        
        main_content = None
        for selector in content_selectors:
            main_content = soup.find('div', selector) or soup.find('main', selector)
            if main_content:
                break
        
        if not main_content:
            main_content = soup.find('body')
        
        if main_content:
            paragraphs = main_content.find_all(['p', 'li', 'h2', 'h3', 'h4'])
            content_parts = []
            
            for elem in paragraphs:
                text = elem.get_text(strip=True)
                if text and len(text) > self.config.MIN_PARAGRAPH_LENGTH:
                    content_parts.append(text)
            
            article_data['content'] = '\n\n'.join(content_parts)
            self.stats['total_chars'] += len(article_data['content'])
            
            headings = main_content.find_all(['h2', 'h3'])
            for heading in headings:
                section = {
                    'heading': heading.get_text(strip=True),
                    'content': []
                }
                
                for sibling in heading.find_next_siblings():
                    if sibling.name in ['h2', 'h3']:
                        break
                    if sibling.name in ['p', 'ul', 'ol']:
                        text = sibling.get_text(strip=True)
                        if text:
                            section['content'].append(text)
                
                if section['content']:
                    article_data['sections'].append(section)
            
            print(f"{Fore.GREEN}   {len(content_parts)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤, {len(article_data['content'])} —Å–∏–º–≤–æ–ª–æ–≤")
        
        return article_data
    
    def sanitize_filename(self, text: str, max_length: int = 100) -> str:
        text = re.sub(r'[^\w\s-]', '', text)
        text = re.sub(r'[-\s]+', '_', text)
        text = re.sub(r'_+', '_', text)
        text = text[:max_length].strip('_')
        return text.lower()
    
    def generate_filename(self, article: Dict, index: int) -> str:
        if article.get('title'):
            base_name = self.sanitize_filename(article['title'])
            if base_name:
                return f"{index:03d}_{base_name}.json"
        
        url_path = urlparse(article['url']).path
        url_parts = [p for p in url_path.split('/') if p]
        if url_parts:
            base_name = self.sanitize_filename(url_parts[-1])
            if base_name:
                return f"{index:03d}_{base_name}.json"
        
        return f"{index:03d}_article.json"
    
    def save_article(self, article: Dict, index: int):
        filename = self.generate_filename(article, index)
        filepath = self.output_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(article, f, ensure_ascii=False, indent=2)
        
        print(f"{Fore.CYAN}üíæ {filename}")
    
    def save_checkpoint(self):
        if not self.config.CHECKPOINT_ENABLED:
            return
        
        if len(self.articles) % self.config.CHECKPOINT_INTERVAL == 0 and len(self.articles) > 0:
            checkpoint_file = f"checkpoint_{len(self.articles)}_articles.json"
            checkpoint_path = self.output_dir / checkpoint_file
            
            with open(checkpoint_path, 'w', encoding='utf-8') as f:
                json.dump(self.articles, f, ensure_ascii=False, indent=2)
            
            elapsed = time.time() - self.stats['start_time']
            rate = (len(self.articles) / elapsed) * 60 if elapsed > 0 else 0
            
            print(f"\n{Fore.YELLOW}üíæ Checkpoint: {len(self.articles)} —Å—Ç–∞—Ç–µ–π ({rate:.1f}/–º–∏–Ω)\n")
    
    def parse_page(self, url: str, depth: int = 0, max_depth: int = None, pbar: tqdm = None):
        if max_depth is None:
            max_depth = self.config.MAX_RECURSION_DEPTH
        
        if url in self.visited_urls or depth > max_depth:
            return
        
        print(f"\n{Fore.CYAN}{'  '*depth}[–ì–ª—É–±–∏–Ω–∞ {depth}/{max_depth}] {url}")
        
        self.visited_urls.add(url)
        
        if len(self.visited_urls) > 1:
            time.sleep(self.config.REQUEST_DELAY)
        
        soup = self.get_page_content(url)
        article = self.extract_article_content(soup, url)
        
        if article and article['content']:
            self.articles.append(article)
            self.stats['total_articles'] += 1
            article_index = len(self.articles)
            
            self.save_article(article, article_index)
            print(f"{Fore.GREEN}‚úÖ –°—Ç–∞—Ç—å—è #{article_index}: {article['title'][:50]}")
            
            self.save_checkpoint()
            
            if pbar:
                pbar.set_postfix({'—Å—Ç–∞—Ç–µ–π': len(self.articles), '—Å–∏–º–≤–æ–ª–æ–≤': f"{self.stats['total_chars']:,}"})
        
        if depth < max_depth:
            links = self.extract_links(soup, url)
            
            if links:
                for link in links:
                    self.parse_page(link, depth + 1, max_depth, pbar)
                    if pbar:
                        pbar.update(1)
    
    def save_final(self):
        final_path = self.output_dir / "all_articles.json"
        
        with open(final_path, 'w', encoding='utf-8') as f:
            json.dump(self.articles, f, ensure_ascii=False, indent=2)
        
        file_size = final_path.stat().st_size
        print(f"{Fore.GREEN}üíæ all_articles.json ({file_size/1024:.1f} KB, {len(self.articles)} —Å—Ç–∞—Ç–µ–π)")
    
    def print_stats(self):
        print(f"\n{Fore.CYAN}{'='*80}")
        print(f"{Fore.CYAN}üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–ê–†–°–ò–ù–ì–ê")
        print(f"{Fore.CYAN}{'='*80}{Style.RESET_ALL}")
        
        print(f"\n{Fore.GREEN}–ó–∞–ø—Ä–æ—Å—ã: {self.stats['successful_requests']}/{self.stats['total_requests']}")
        print(f"{Fore.BLUE}–°—Å—ã–ª–∫–∏: {self.stats['total_links_found']} –Ω–∞–π–¥–µ–Ω–æ, {len(self.visited_urls)} –ø–æ—Å–µ—â–µ–Ω–æ")
        print(f"{Fore.MAGENTA}–°—Ç–∞—Ç—å–∏: {self.stats['total_articles']} ({self.stats['total_chars']:,} —Å–∏–º–≤–æ–ª–æ–≤)")
        
        if self.failed_urls:
            print(f"{Fore.RED}–û—à–∏–±–∫–∏: {len(self.failed_urls)} URL")
        
        print(f"{Fore.CYAN}{'='*80}\n")
    
    def run(self):
        start_time = time.time()
        self.stats['start_time'] = start_time
        
        with tqdm(total=0, desc="–ü—Ä–æ–≥—Ä–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞", unit="—Å—Ç—Ä") as pbar:
            self.parse_page(self.config.BASE_URL, pbar=pbar)
        
        elapsed = time.time() - start_time
        
        print(f"\n{Fore.GREEN}{'='*80}")
        print(f"{Fore.GREEN}‚úÖ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù –∑–∞ {elapsed:.1f} —Å–µ–∫ ({elapsed/60:.1f} –º–∏–Ω)")
        print(f"{Fore.GREEN}{'='*80}{Style.RESET_ALL}")
        
        self.print_stats()
        self.save_final()
        
        return self.articles


# ============================================================================
# –û–ß–ò–°–¢–ö–ê –î–£–ë–õ–ò–ö–ê–¢–û–í
# ============================================================================

class DirectoryCleaner:
    def __init__(self, directory: Path):
        self.directory = directory
        self.stats = {
            'duplicates_removed': 0,
            'files_renamed': 0,
            'checkpoints_removed': 0,
            'numbered_removed': 0
        }
    
    def remove_duplicates(self):
        print(f"\n{Fore.CYAN}{'='*80}")
        print(f"{Fore.CYAN}üßπ –®–ê–ì 1: –£–î–ê–õ–ï–ù–ò–ï –î–£–ë–õ–ò–ö–ê–¢–û–í")
        print(f"{Fore.CYAN}{'='*80}{Style.RESET_ALL}\n")
        
        files_by_url = {}
        
        for filepath in self.directory.glob("*.json"):
            if filepath.name.startswith(('checkpoint_', 'all_')):
                continue
            
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    url = data.get('url', '')
                    
                    if url:
                        if url in files_by_url:
                            filepath.unlink()
                            self.stats['duplicates_removed'] += 1
                            print(f"{Fore.RED}‚úì –£–¥–∞–ª–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç: {filepath.name}")
                        else:
                            files_by_url[url] = filepath
            except Exception as e:
                print(f"{Fore.RED}–û—à–∏–±–∫–∞: {filepath.name} - {e}")
        
        print(f"\n{Fore.GREEN}–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {self.stats['duplicates_removed']}\n")
    
    def remove_numbering(self):
        print(f"{Fore.CYAN}{'='*80}")
        print(f"{Fore.CYAN}üîÑ –®–ê–ì 2: –°–û–ó–î–ê–ù–ò–ï –§–ê–ô–õ–û–í –ë–ï–ó –ù–£–ú–ï–†–ê–¶–ò–ò")
        print(f"{Fore.CYAN}{'='*80}{Style.RESET_ALL}\n")
        
        for filepath in self.directory.glob("*.json"):
            if filepath.name.startswith(('checkpoint_', 'all_')):
                continue
            
            filename = filepath.name
            
            if len(filename) > 4 and filename[:3].isdigit() and filename[3] == '_':
                new_name = filename[4:]
                new_path = filepath.parent / new_name
                
                if not new_path.exists():
                    shutil.copy2(filepath, new_path)
                    self.stats['files_renamed'] += 1
                    print(f"{Fore.GREEN}‚úì –°–æ–∑–¥–∞–Ω: {new_name}")
        
        print(f"\n{Fore.GREEN}–°–æ–∑–¥–∞–Ω–æ —Ñ–∞–π–ª–æ–≤ –±–µ–∑ –Ω—É–º–µ—Ä–∞—Ü–∏–∏: {self.stats['files_renamed']}\n")
    
    def remove_numbered_files(self):
        print(f"{Fore.CYAN}{'='*80}")
        print(f"{Fore.CYAN}üóëÔ∏è  –®–ê–ì 3: –£–î–ê–õ–ï–ù–ò–ï –ü–†–û–ù–£–ú–ï–†–û–í–ê–ù–ù–´–• –§–ê–ô–õ–û–í")
        print(f"{Fore.CYAN}{'='*80}{Style.RESET_ALL}\n")
        
        for filepath in list(self.directory.glob("*.json")):
            if filepath.name.startswith(('checkpoint_', 'all_')):
                continue
            
            filename = filepath.name
            
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 3 —Ü–∏—Ñ—Ä –∏ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è - —É–¥–∞–ª—è–µ–º
            if len(filename) > 4 and filename[:3].isdigit() and filename[3] == '_':
                filepath.unlink()
                self.stats['numbered_removed'] += 1
                print(f"{Fore.RED}‚úì –£–¥–∞–ª–µ–Ω: {filename}")
        
        print(f"\n{Fore.GREEN}–£–¥–∞–ª–µ–Ω–æ –ø—Ä–æ–Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {self.stats['numbered_removed']}\n")
    
    def remove_checkpoints(self):
        print(f"{Fore.CYAN}{'='*80}")
        print(f"{Fore.CYAN}üóëÔ∏è  –®–ê–ì 4: –£–î–ê–õ–ï–ù–ò–ï –ß–ï–ö–ü–û–ò–ù–¢–û–í")
        print(f"{Fore.CYAN}{'='*80}{Style.RESET_ALL}\n")
        
        for filepath in self.directory.glob("checkpoint_*.json"):
            filepath.unlink()
            self.stats['checkpoints_removed'] += 1
            print(f"{Fore.GREEN}‚úì –£–¥–∞–ª–µ–Ω: {filepath.name}")
        
        if self.stats['checkpoints_removed'] == 0:
            print(f"{Fore.YELLOW}–ß–µ–∫–ø–æ–∏–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        else:
            print(f"\n{Fore.GREEN}–£–¥–∞–ª–µ–Ω–æ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤: {self.stats['checkpoints_removed']}")
        
        print()
    
    def verify_cleanup(self):
        print(f"{Fore.CYAN}{'='*80}")
        print(f"{Fore.CYAN}‚úÖ –ü–†–û–í–ï–†–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–ê")
        print(f"{Fore.CYAN}{'='*80}{Style.RESET_ALL}\n")
        
        all_files = [f for f in self.directory.glob("*.json") 
                    if not f.name.startswith(('checkpoint_', 'all_'))]
        
        numbered_files = [f for f in all_files 
                         if len(f.name) > 4 and f.name[:3].isdigit() and f.name[3] == '_']
        
        print(f"{Fore.GREEN}–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(all_files)}")
        
        if numbered_files:
            print(f"{Fore.RED}‚ö†Ô∏è  –û–®–ò–ë–ö–ê: –û—Å—Ç–∞–ª–∏—Å—å –ø—Ä–æ–Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã: {len(numbered_files)}")
            for f in numbered_files[:10]:
                print(f"{Fore.RED}  ‚Ä¢ {f.name}")
            if len(numbered_files) > 10:
                print(f"{Fore.RED}  ... –∏ –µ—â–µ {len(numbered_files) - 10}")
        else:
            print(f"{Fore.GREEN}‚úÖ –ü—Ä–æ–Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å!")
            print(f"{Fore.GREEN}‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã –∏–º–µ—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è!")
        
        print(f"\n{Fore.CYAN}{'='*80}\n")
    
    def print_stats(self):
        print(f"{Fore.CYAN}{'='*80}")
        print(f"{Fore.CYAN}üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ß–ò–°–¢–ö–ò")
        print(f"{Fore.CYAN}{'='*80}{Style.RESET_ALL}\n")
        print(f"{Fore.GREEN}  ‚Ä¢ –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {self.stats['duplicates_removed']}")
        print(f"{Fore.GREEN}  ‚Ä¢ –°–æ–∑–¥–∞–Ω–æ —Ñ–∞–π–ª–æ–≤ –±–µ–∑ –Ω—É–º–µ—Ä–∞—Ü–∏–∏: {self.stats['files_renamed']}")
        print(f"{Fore.GREEN}  ‚Ä¢ –£–¥–∞–ª–µ–Ω–æ –ø—Ä–æ–Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã—Ö: {self.stats['numbered_removed']}")
        print(f"{Fore.GREEN}  ‚Ä¢ –£–¥–∞–ª–µ–Ω–æ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤: {self.stats['checkpoints_removed']}")
        print(f"\n{Fore.CYAN}{'='*80}\n")
    
    def run(self):
        self.remove_duplicates()
        self.remove_numbering()
        self.remove_numbered_files()
        self.remove_checkpoints()
        self.verify_cleanup()
        self.print_stats()


# ============================================================================
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# ============================================================================

def main():
    print(f"\n{Fore.CYAN}{'='*80}")
    print(f"{Fore.CYAN}T-BANK KNOWLEDGE BASE PARSER & CLEANER")
    print(f"{Fore.CYAN}{'='*80}{Style.RESET_ALL}\n")
    
    # –≠—Ç–∞–ø 1: –ü–∞—Ä—Å–∏–Ω–≥
    print(f"{Fore.YELLOW}üì• –≠–¢–ê–ü 1: –ü–ê–†–°–ò–ù–ì\n")
    parser = TBankKnowledgeParser(config=ParserConfig)
    articles = parser.run()
    
    # –≠—Ç–∞–ø 2: –û—á–∏—Å—Ç–∫–∞
    print(f"\n{Fore.YELLOW}üßπ –≠–¢–ê–ü 2: –û–ß–ò–°–¢–ö–ê\n")
    cleaner = DirectoryCleaner(parser.output_dir)
    cleaner.run()
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"{Fore.GREEN}{'='*80}")
    print(f"{Fore.GREEN}üéâ –í–°–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print(f"{Fore.GREEN}{'='*80}{Style.RESET_ALL}\n")
    print(f"{Fore.CYAN}üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {parser.output_dir.absolute()}")
    print(f"{Fore.CYAN}üìö –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π: {len(articles)}\n")


if __name__ == "__main__":
    main()
